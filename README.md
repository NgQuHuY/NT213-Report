
# NT213-Report

_"This is my own report about the work i have done in this project. The group's report was summitted on my school's website."_


## 1)	Data preprocessing 
The article does not specify how to handle and tokenize the data in the datasets, and the data preprocess part of the article is also not shared publicly. Based on the article, i only know these following informations:

- Keras word tokenizer and Keras word embedding are used to tokenize words into sequences.
- 3000 frequent terms are the best vocabulary size for the tokenization process.
- Post-padding is used to balance the sequences.
- All fields in the HTTP request are used.
- Special characters will not be removed.
Since there are no details on how to split the data into words for tokenization, i referred to other relevant articles and proposed a processing model as follows:

- Step 1: The CSIC 2010 dataset will still use all fields in the request. However, for the PKDD 2007 dataset, the number of fields is too large, and some fields do not contain attack patterns, ecspecially in the HTTP header, only a few fields contain necessary patterns. Thus, i think it might improve the speed and accuracy of the model if it had been correctly selected. According to [1], which is a CSV dataset have been extracted and reduced from the XML one, the selected fields include _'method', 'uri', 'query', 'protocol', 'host', 'connection', 'accept', 'accept_charset', 'accept_language', 'cache_control', 'cookie', 'pragma', 'user_agent', 'content_length', 'content_type'_. The code to handle XML dataset of PKDD is in the file "emcl_Data", which is based on [2]

- Step 2: Special characters in the dataset, especially in abnormal requests, mostly have URL encoding and '+' to bypass some server's built-in defense mechanisms. In addition, some characters only act as delimiters to separate words, so they can be split and removed. According to [3], words will be split by whitespace, '+' and URL encoding. In [4], URL decoding, lowercase, and split by "/,?,&,=,+,_"whitespace"_ " are performed, which can retain some special characters that have been encoded, and lowercase them can help to synchronize uppercase characters, reducing the number of cases where some frequent words, which could be the attack patterns, are unfortunately removed due to case sensitivity. Besides, the number of '+' characters is often not fixed and mostly acts as a whitespace. Therefore, i decide not to keep it as a special character. Finally, i use the processing algorithm in [4] for the data preprocess part. The '+' and other special characters which are considerated as "split delimiters", will be removed all and replaced with whitespace, and then it's able for Keras.tokenizer. In addition, two characters ';' and ',' are also added to the split character set.

<p align="center">
  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/61856218-876b-4201-943f-3b4ac1c2acfe" />
</p>
<p align="center">
  <i>An example of an SQLi attack pattern in paper [2].</i>
</p>

<p align="center">
  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/94f07643-11f7-4bc2-bd3c-24a65d775a63" />
</p>
<p align="center">
  <i>An example of an attack pattern after preprocessing pharse.</i>
</p>

## 2)	Data augmentaion Algorithm 

The DaSANA algorithm, which is used for creating augmentaion data based on HTTP length,  is implemented based on the algorithm in the paper, including these following steps:

- Calculate the Normalize value LNormal of the length of each HTTP request using L – Lmin / Lmax – Lmin.
- Create an array Ci with the same length as an original sequence Si using numpy.random.binomial with n = 1 and p = LNormal * 0.05 to ensure that 20% of Ci have a value of 1.
- Create an array Ni with the same length as Si using numpy.random.normal with Mean = (f + 2f) / 2 and Standard Deviation = (2f - f) / 4 (f is the number of frequent words used in the tokenize process).
- A noise sequence NDi is generated by Ni * Ci + Si * Ci' .
- The set of noise sequences NDi are added to the set of original sequence Si to create a new sequence set for training and testing, with a size of 2 * len(Si).

However, the DaSANA algorithm implementation has a modification as follows: The values in the Ni array, which is declared in the paper, must be within the range (k,2k). But to find the exact Standard Deviation value for the random values to fall within this range is almost impossible, so i only use the Range rule formula [5] to find an approximate range of (k,2k).

<p align="center">
  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/bdc328a2-6a5b-4f75-85e0-92cd42aab450" />
</p>
<p align="center">
  <i>Data Augmentation algorithm (DaSANA) to create the set of augmentation sequences .</i>
</p>

## 3)   Model
I have my friend working on building the percentage-split model for training, the following is his report for the model : 
The model is built using the Sequential API of Keras.
- The first layer is an Embedding layer, which maps each character in the input string to a dense vector representation. The embedding_dim parameter is set to match the paper (16 for binary and 32 for multiclass).
- The next layer is a Bi-LSTM layer. The Bidirectional wrapper allows the LSTM layer to process the input sequence in both forward and backward directions simultaneously. This helps capture dependencies in both directions. The parameters are set according to the paper:
    - units=128: The number of hidden units/neurons in the LSTM layer.
    - activation='tanh': The activation function used by the LSTM units.
    - dropout=0.5: Specifies the dropout rate for the input units, to prevent overfitting.
    - recurrent_dropout=0.5: Specifies the dropout rate for the recurrent connections.
- Another Bi-LSTM layer follows with the same parameters as the first layer.
- Finally, a Dense layer with softmax activation function is added. This layer generates class probabilities for each class.
- The model is compiled with the Adam optimizer and uses sparse categorical cross-entropy loss function for multiclass and binary_crossentropy for binary classification.


## 4)   Result

There is a mistake that i used the augmented data, not the orginal one, to test the model. So the result may change a little bit. But when i checked with the orginal data, the difference was not considerable, which is nearly -0.01 or lower. So i still use the mistake one to report. 
#### CSIC Dataset (Binary classifcation)
<p align="center">
  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/103ecaba-5e8c-4f02-b315-62fb23b18301" />

  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/4dc93cea-6913-47bd-a336-87fc43198f88" />
</p>

#### PKDD Dataset (Binary classifcation)
<p align="center">
  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/9228b79f-8d2d-44a3-8a9e-915bd277886c" />

  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/e806626e-32f0-4e0d-a076-8eb6e33b5035" />
</p>

#### PKDD Dataset (Multi-label classifcation)

<p align="center">
  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/ecf1ce2b-6136-4d9b-9428-eecce5a579fa" />

  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/251c088a-2d5e-428a-b17f-7afaa8d8f48e" />

  <img src="https://github.com/NgQuHuY/NT213-Report/assets/105098386/c2e17296-3054-4e3c-8694-133d8859f712" />
</p>

The result is quite surprising. 
- At the binary classifcation, the ACC of CSIC was more lower than the paper's one, which is nearly 0.03 difference. While the ACC of EMCL was just a slight difference of 0.01 lower. There is a concern that the TN rate of CSIC and FP rate of PKDD is higher than usual, which may be the reason the decrease of result.
 - By contract, the multi-classifcation is the improvement with 0.011 higher than paper's ACC, the confusion of OS com, SSI, Path Trav as well as other classes is fixed. But the TN of Valid label still remain a high value.

## Paper
[ H. Karacan and M. Sevri, "A Novel Data Augmentation Technique and Deep Learning Model for Web Application Security," in IEEE Access, vol. 9, pp. 150781-150797, 2021](https://doi.org/10.1109/ACCESS.2021.3125785)
## Reference
 - [[1] : Web-Application-Attack-Datasets](https://github.com/msudol/Web-Application-Attack-Datasets/tree/c37152715bf95776bfb8d3430e38c3462914068a/OriginalDataSets)
  - [[2] : Tranform ECML-PKDD.xml dataset ](https://github.com/rashimo/ChCNN/blob/master/ecml_pkdd/tranform_ECMLPKDD.ipynb)
 - [[3] : Classification of HTTP Attacks : A Study on the ECML/PKDD 2007 Discovery Challenge](https://doi.org/10.2172/1113394)
 - [[4] : DeepWAF: Detecting Web Attacks Based on CNN and LSTM Models, Kuang, X. et al. (2019). In: Vaidya, J., Zhang, X., Li, J. (eds) Cyberspace Safety and Security. CSS 2019. Lecture Notes in Computer Science(), vol 11983. Springer, Cham. ](https://doi.org/10.1007/978-3-030-37352-8_11)
 - [[5] : Range Rule for Standard Deviation ](https://www.thoughtco.com/range-rule-for-standard-deviation-3126231#:~:text=The%20range%20rule%20tells%20us,estimate%20of%20the%20standard%20deviation.)

